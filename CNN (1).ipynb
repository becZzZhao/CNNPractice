{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ8ZSPv5Xshy",
        "colab_type": "text"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "This notebook contains several exercises to practice regularization.  Each network below contains instructions to practice regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-12-26T19:58:03.252675Z",
          "start_time": "2018-12-26T19:58:00.632733Z"
        },
        "id": "97YADQAzXsh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.datasets import boston_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGJyu-dQXsiC",
        "colab_type": "text"
      },
      "source": [
        "**bold text**# Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKd4ZvQYXsiE",
        "colab_type": "text"
      },
      "source": [
        "Let's see how well you can improve boston housing test MAE using only a subset of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-12-26T20:13:37.811236Z",
          "start_time": "2018-12-26T20:13:37.799561Z"
        },
        "id": "Ern3h0VKXsiG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "01c174df-69af-4a19-f7a2-d8e57c8a7b00"
      },
      "source": [
        "\n",
        "# Hypertuning:\n",
        "# the best parameter according to hypertuning is : batch size = 20, epoch = 60. \n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
        "\n",
        "np.random.seed(0)\n",
        "index = np.arange(len(x_train))\n",
        "\n",
        "# Take a biased training set - top 50 house prices\n",
        "index = sorted(index, key=lambda x: -y_train[x])\n",
        "x_train = x_train[index]\n",
        "y_train = y_train[index]\n",
        "\n",
        "sample = 50\n",
        "x_train, y_train = x_train[:sample], y_train[:sample]\n",
        "x_test, y_test = x_test, y_test\n",
        "\n",
        "scalar = StandardScaler()\n",
        "x_train2 = scalar.fit_transform(x_train)\n",
        "x_test2 = scalar.transform(x_test)\n",
        "\n",
        "print(\"Training Dataset Size: \", x_train2.shape)\n",
        "print(\"Training Labels Size: \", y_train.shape)\n",
        "print(\"Testing Dataset Size: \", x_test2.shape)\n",
        "print(\"Testing Labels Size: \", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Size:  (50, 13)\n",
            "Training Labels Size:  (50,)\n",
            "Testing Dataset Size:  (102, 13)\n",
            "Testing Labels Size:  (102,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6DxpBlAlUWE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-12-26T20:17:06.018273Z",
          "start_time": "2018-12-26T20:17:05.910265Z"
        },
        "id": "eYmvI1UwXsiU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f0dd0a6-dd57-47b0-8402-a215ccf8fbe3"
      },
      "source": [
        "# /***************************************************************************************\n",
        "# *    Title: How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras\n",
        "# *    Author:  Jason Brownlee\n",
        "# *    Date: 2016-08-09\n",
        "# *    Code version: N.A.\n",
        "# *    Availability: <https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/>\n",
        "# *\n",
        "# ***************************************************************************************/\n",
        "import numpy\n",
        "\n",
        "def createModel(neurons = 1, dropout_rate = 0.2):\n",
        "  model1 = Sequential()\n",
        "  model1.add(Dense(neurons, activation='relu',input_shape=(13,)))\n",
        "  # model1.add(Dense(neurons, activation='relu'))\n",
        "  model1.add(Dense(neurons, activation='relu'))\n",
        "  model1.add(Dense(1,))\n",
        "  #model1.compile(loss='mae', optimizer=Adam(lr=0.01), metrics=['accuracy'])\n",
        "  model1.compile(loss='mae', optimizer=Adam(lr=0.01))\n",
        "  return model1\n",
        "\n",
        "\n",
        "print('a')\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "model = KerasRegressor(build_fn=createModel, verbose=1)\n",
        "\n",
        "batch_size = [20, 40, 60]\n",
        "epochs = [20, 30]\n",
        "neurons =  [15]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs, neurons = neurons)\n",
        "print(\"d\")\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "\n",
        "print('c')\n",
        "grid_result = grid.fit(x_test2, y_test)\n",
        "\n",
        "\n",
        "print('b')\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a\n",
            "d\n",
            "c\n",
            "Epoch 1/20\n",
            "68/68 [==============================] - 13s 195ms/step - loss: 23.8844\n",
            "Epoch 2/20\n",
            "68/68 [==============================] - 0s 431us/step - loss: 21.8213\n",
            "Epoch 3/20\n",
            "68/68 [==============================] - 0s 417us/step - loss: 19.9204\n",
            "Epoch 4/20\n",
            "68/68 [==============================] - 0s 393us/step - loss: 18.2978\n",
            "Epoch 5/20\n",
            "68/68 [==============================] - 0s 442us/step - loss: 16.7801\n",
            "Epoch 6/20\n",
            "68/68 [==============================] - 0s 449us/step - loss: 15.6263\n",
            "Epoch 7/20\n",
            "68/68 [==============================] - 0s 415us/step - loss: 14.3433\n",
            "Epoch 8/20\n",
            "68/68 [==============================] - 0s 476us/step - loss: 12.9209\n",
            "Epoch 9/20\n",
            "68/68 [==============================] - 0s 490us/step - loss: 11.9083\n",
            "Epoch 10/20\n",
            "68/68 [==============================] - 0s 458us/step - loss: 11.2685\n",
            "Epoch 11/20\n",
            "68/68 [==============================] - 0s 500us/step - loss: 10.8061\n",
            "Epoch 12/20\n",
            "68/68 [==============================] - 0s 422us/step - loss: 10.2770\n",
            "Epoch 13/20\n",
            "68/68 [==============================] - 0s 411us/step - loss: 9.6036\n",
            "Epoch 14/20\n",
            "68/68 [==============================] - 0s 467us/step - loss: 8.9189\n",
            "Epoch 15/20\n",
            "68/68 [==============================] - 0s 427us/step - loss: 8.4489\n",
            "Epoch 16/20\n",
            "68/68 [==============================] - 0s 441us/step - loss: 7.7426\n",
            "Epoch 17/20\n",
            "68/68 [==============================] - 0s 525us/step - loss: 7.0423\n",
            "Epoch 18/20\n",
            "68/68 [==============================] - 0s 480us/step - loss: 6.5626\n",
            "Epoch 19/20\n",
            "68/68 [==============================] - 0s 477us/step - loss: 5.9985\n",
            "Epoch 20/20\n",
            "68/68 [==============================] - 0s 493us/step - loss: 5.3660\n",
            "34/34 [==============================] - 5s 162ms/step\n",
            "Epoch 1/20\n",
            "68/68 [==============================] - 13s 195ms/step - loss: 23.4807\n",
            "Epoch 2/20\n",
            "68/68 [==============================] - 0s 413us/step - loss: 21.6337\n",
            "Epoch 3/20\n",
            "68/68 [==============================] - 0s 384us/step - loss: 20.5750\n",
            "Epoch 4/20\n",
            "68/68 [==============================] - 0s 374us/step - loss: 19.4828\n",
            "Epoch 5/20\n",
            "68/68 [==============================] - 0s 346us/step - loss: 18.6375\n",
            "Epoch 6/20\n",
            "68/68 [==============================] - 0s 346us/step - loss: 17.8277\n",
            "Epoch 7/20\n",
            "68/68 [==============================] - 0s 352us/step - loss: 16.4457\n",
            "Epoch 8/20\n",
            "68/68 [==============================] - 0s 349us/step - loss: 15.2200\n",
            "Epoch 9/20\n",
            "68/68 [==============================] - 0s 297us/step - loss: 14.7898\n",
            "Epoch 10/20\n",
            "68/68 [==============================] - 0s 346us/step - loss: 13.6341\n",
            "Epoch 11/20\n",
            "68/68 [==============================] - 0s 400us/step - loss: 12.5489\n",
            "Epoch 12/20\n",
            "68/68 [==============================] - 0s 382us/step - loss: 11.5654\n",
            "Epoch 13/20\n",
            "68/68 [==============================] - 0s 305us/step - loss: 10.5756\n",
            "Epoch 14/20\n",
            "68/68 [==============================] - 0s 389us/step - loss: 9.9387\n",
            "Epoch 15/20\n",
            "68/68 [==============================] - 0s 386us/step - loss: 9.7676\n",
            "Epoch 16/20\n",
            "68/68 [==============================] - 0s 321us/step - loss: 9.2270\n",
            "Epoch 17/20\n",
            "68/68 [==============================] - 0s 357us/step - loss: 8.6926\n",
            "Epoch 18/20\n",
            "68/68 [==============================] - 0s 361us/step - loss: 8.2806\n",
            "Epoch 19/20\n",
            "68/68 [==============================] - 0s 370us/step - loss: 7.8693\n",
            "Epoch 20/20\n",
            "68/68 [==============================] - 0s 325us/step - loss: 7.5017\n",
            "34/34 [==============================] - 5s 161ms/step\n",
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e05a72e8a8ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2959\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhYGpIXpVUhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Dropout(myPct[i], input_shape=(1,)))\n",
        "model2.add(Dense(15, activation=\"relu\"))\n",
        "model2.add(Dense(1))\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "model2.compile(loss='mae', optimizer=Adam(lr=0.01))\n",
        "model2.evaluate(x_test2, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8cFbJ-GVVis",
        "colab_type": "text"
      },
      "source": [
        "## Drop Out Input Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCB-w6iuVZNa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3bf2e6c2-eff5-4630-be92-40e125b2ec60"
      },
      "source": [
        "# Improve Variance by adding a drop-out layer.\n",
        "\n",
        "from keras import regularizers\n",
        "\n",
        "loss = []\n",
        "import numpy\n",
        "\n",
        "myPct = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "for i in range(0, len(myPct)):\n",
        "  \n",
        "\n",
        "\n",
        "    model33 = Sequential()\n",
        "\n",
        "    model33.add(Dropout(myPct[i], input_shape=(13,)))\n",
        "    model33.add(Dense(15, activation=\"relu\"))\n",
        "    model33.add(Dense(1))\n",
        "\n",
        "    model33.summary()\n",
        "\n",
        "    model33.compile(loss='mae', optimizer=Adam(lr=0.01))\n",
        "    callbacks = []\n",
        "    history = model33.fit(x_train2, y_train, batch_size=20, epochs=60, callbacks=callbacks, verbose=0)\n",
        "\n",
        "    model33.evaluate(x_test2, y_test)\n",
        "\n",
        "    myLoss = model33.evaluate(x_test2, y_test)\n",
        "    loss.append(myLoss)\n",
        "    print(loss)\n",
        "\n",
        "\n",
        "\n",
        "print(myPct)\n",
        "print(loss)\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "  \n",
        "plt.plot(myPct, loss)\n",
        "print(min(loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_243\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_3 (Dropout)          (None, 13)                0         \n",
            "_________________________________________________________________\n",
            "dense_725 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_726 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 54ms/step\n",
            "102/102 [==============================] - 0s 207us/step\n",
            "[20.72174779106589]\n",
            "Model: \"sequential_244\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_4 (Dropout)          (None, 13)                0         \n",
            "_________________________________________________________________\n",
            "dense_727 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_728 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 54ms/step\n",
            "102/102 [==============================] - 0s 244us/step\n",
            "[20.72174779106589, 15.323028377458161]\n",
            "Model: \"sequential_245\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_5 (Dropout)          (None, 13)                0         \n",
            "_________________________________________________________________\n",
            "dense_729 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_730 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 54ms/step\n",
            "102/102 [==============================] - 0s 165us/step\n",
            "[20.72174779106589, 15.323028377458161, 22.187451717900295]\n",
            "Model: \"sequential_246\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_6 (Dropout)          (None, 13)                0         \n",
            "_________________________________________________________________\n",
            "dense_731 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_732 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 54ms/step\n",
            "102/102 [==============================] - 0s 206us/step\n",
            "[20.72174779106589, 15.323028377458161, 22.187451717900295, 17.76308140100217]\n",
            "Model: \"sequential_247\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_7 (Dropout)          (None, 13)                0         \n",
            "_________________________________________________________________\n",
            "dense_733 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_734 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 54ms/step\n",
            "102/102 [==============================] - 0s 245us/step\n",
            "[20.72174779106589, 15.323028377458161, 22.187451717900295, 17.76308140100217, 20.356137070001342]\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_248\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_8 (Dropout)          (None, 13)                0         \n",
            "_________________________________________________________________\n",
            "dense_735 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_736 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 55ms/step\n",
            "102/102 [==============================] - 0s 172us/step\n",
            "[20.72174779106589, 15.323028377458161, 22.187451717900295, 17.76308140100217, 20.356137070001342, 17.500648292840697]\n",
            "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_249\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_9 (Dropout)          (None, 13)                0         \n",
            "_________________________________________________________________\n",
            "dense_737 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_738 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 55ms/step\n",
            "102/102 [==============================] - 0s 206us/step\n",
            "[20.72174779106589, 15.323028377458161, 22.187451717900295, 17.76308140100217, 20.356137070001342, 17.500648292840697, 21.298247393439798]\n",
            "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_250\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_10 (Dropout)         (None, 13)                0         \n",
            "_________________________________________________________________\n",
            "dense_739 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_740 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 55ms/step\n",
            "102/102 [==============================] - 0s 199us/step\n",
            "[20.72174779106589, 15.323028377458161, 22.187451717900295, 17.76308140100217, 20.356137070001342, 17.500648292840697, 21.298247393439798, 13.321239527534036]\n",
            "WARNING:tensorflow:Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"sequential_251\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout_11 (Dropout)         (None, 13)                0         \n",
            "_________________________________________________________________\n",
            "dense_741 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_742 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 55ms/step\n",
            "102/102 [==============================] - 0s 182us/step\n",
            "[20.72174779106589, 15.323028377458161, 22.187451717900295, 17.76308140100217, 20.356137070001342, 17.500648292840697, 21.298247393439798, 13.321239527534036, 11.81053907731]\n",
            "[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
            "[20.72174779106589, 15.323028377458161, 22.187451717900295, 17.76308140100217, 20.356137070001342, 17.500648292840697, 21.298247393439798, 13.321239527534036, 11.81053907731]\n",
            "11.81053907731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd3xUdb7/8dd3UiGVNAiZkEIJvYSg\nQaTZRcWFWNe6l5XrrmXVVXe9/u7e7XdXXV37XtvquoqioGIXJQgooKG3JISeEJgkEJKQQpL5/v6Y\niURMyGQyM2fO5PN8PHjcYeZkzueO7DtnPudblNYaIYQQ5mMxugAhhBDukQAXQgiTkgAXQgiTkgAX\nQgiTkgAXQgiTCvblyRISEnR6erovTymEEKa3bt26Sq114qnP+zTA09PTKSgo8OUphRDC9JRS+zp6\nXlooQghhUhLgQghhUhLgQghhUhLgQghhUhLgQghhUhLgQghhUhLgQghhUhLgAehgdQNLNh00ugwh\nhJf5dCKP8I0nl5Ww4Jv9ZMRHMMYaY3Q5Qggv6fIKXCmVqpTKV0ptV0ptU0r9wvn8w0qpQqXUZqXU\nO0qpWO+XK7qitWZ5kQ2A51fuNrgaIYQ3udJCaQF+qbUeCeQCtymlRgJLgdFa67FAMfCA98oUrio8\nVEv5sUZS4/rw4ZZyyqobjC5JCOElXQa41rpca73e+bgW2AGkaK0/01q3OA9bA1i9V6Zw1bJCx9X3\nk9dmA/DPVXuMLEcI4UXduomplEoHJgBrT3npP4CPO/mZ+UqpAqVUQUVFhTs1im5YXmRjdEo041Nj\nuXRsMm98e4CaxmajyxJCeIHLAa6UigQWAXdprWvaPf8gjjbLax39nNb6Oa11jtY6JzHxB6shCg+q\nrj/Bun1HOScrCYBbpmZS19TCG9/sN7gyIYQ3uBTgSqkQHOH9mtZ6cbvnbwYuBa7Tsr294VbsrMSu\nYcZwR4CPTolhcmY8//xqL82tdoOrE0J4miujUBTwIrBDa/1ou+cvAu4HZmut671XonBVfqGNuIhQ\nxllPDgi6ZVoG5cca+XBzuYGVCSG8wZUr8CnADcA5SqmNzj+zgKeAKGCp87l/eLNQcXqtds2XxRVM\nH5ZIkEV99/yMYUkMSYrkuRW7kS9JQgSWLifyaK1XAaqDlz7yfDnCXZtKqzly/AQzne2TNhaL4pap\nGfxq0RZW76rirCEJBlUohPA0mUofIJYX2rAomDb0hwF9+fgUEiJDeU4m9ggRUCTAA8SyIhsT0/oR\n2zf0B6+FhwRx0+R0lhdVUHy41oDqhBDeIAEeAGw1jWwtq2FGVlKnx1yfm0Z4iIUX5CpciIAhAR4A\nlhc5JkidM7zzAO8XEcqVE1N5d8NBbLWNvipNCOFFEuABIL/IRnJMOMMHRJ32uHlnZ9Bst/Ovr/f5\nqDIhhDdJgJvciRY7K3dWMiMrCceQ/c6lJ0Rwwcj+vLpmH/UnWk57rBDC/0mAm1zBviPUNbUwM8u1\nZQpumZrJsYZm3l5X6uXKRKA6Vt/Ms8t3yexePyABbnL5hTZCgyxMcXF898S0fkwYFMsLK/fQapeJ\nPaL7/ra0iL9+UsjKnbI4ndEkwE0uv6iCMzPjiAhzbXMlpRTzp2ay/0g9S7cf8nJ1ItAcOFLPAufi\naGt2HzG4GiEBbmIHjtRTYqtj5mmGD3bkglEDGBTXl+dWyJBC0T2PfV6MRSmGJkWyZneV0eX0ehLg\nJpbv3Drt1OnzXQmyKOadncH6/dWs2ydXUcI1xYdreWdDGTedlc7FY5LZWnZM1po3mAS4iS0rtJGR\nEEFGQkS3f/bKHCsxfUJ4foXs2CNc87fPiogIDebW6YPJzYzDrqFgr1wAGEkC3KQaTrSyelcVM1wc\nfXKqvqHBXJ87iE+3H2Jf1XEPVycCzcYD1Xy67TC3TM0kLiKU7EH9CA22SB/cYBLgJrVmdxVNLfbT\nzr7syk2T0wmxWHhR9s0UXXj400LiIkKZNzUDcKyvMyE1VvrgBpMAN6llhTb6hgZxRkac2++RFB3O\n5eMHsrDgAEePn/BgdSKQfFVSyVclVfx8xmAi2412ys2Mlz64wSTATUhrTX6RjSlDEggLDurRe90y\nLZPGZjuvrZXp9eKHtNY89GkRA2PCuT437Xuv5WbGSx/cYBLgJlRiq6P0aEO3hw92ZFj/KKYPS+Tl\nr/fR2NzqgepEIFm6/TCbDlTzi/OGEh7y/YuFCYNipQ9uMFME+Edbyvnfj3YYXYbfODl80L0bmKea\nPy2Tyromlmw86JH3E4Gh1a555LMiMhMiyMu2/uD1tj746l3SBzeKKQJ8R3kNz6/czaFjsgwqOPrf\nwwdEkRzTxyPvd9bgeEYkR/P8Stk3U5z03sYyig/Xcc8FwwgO6jgqcjPj2XbwGMcapA9uBFME+JwJ\nKdg1vLOhzOhSDFfT2EzB3qM9Gn1yKqUU86dlsNNWx/JiWd9COFa5fOzzYkYNjGbW6OROj5M+uLFM\nEeCZiZFkD4pl8frSXn+FuGpnJS123e3Zl125dOxABkSH87xMrxfAm9/u58CRBu69MAuLpfNlik/2\nwaWNYgRTBDhA3kQrO211bCk7ZnQphsovtBHTJ4QJqbEefd+QIAs/mZLO17uq2NrLP+Perv5EC08s\nK+GM9DhmDDv9fZaT48HlCtwIpgnwS8cMJDTYwqJevI613a7JL6pg2rDETnuSPXHtmYOIDAuWfTN7\nuVe+3kdFbRP3XZTV5SYhIH1wI5kmwGP6hnD+iP4s2XSQEy29cyH5bQdrqKxrcnnzhu6KDg/h6kmp\nvL+5nIPVDV45h/Bvxxqa+ceXu5iZlcikdNcmiU0eLH1wo5gmwAHyJqZwtL75u2F0vc2yQhtKwfQu\nvtb2xE+mpAPw8td7vXYOf7O38jhPfrGz114YtPf8it0ca2jm3guzXP6Z8anSBzeKqQJ82tBEEiJD\ne20bJb/IxjhrLPGRYV47h7VfX2aNSWbB2v3U9oIp0nVNLcx75Vv+trSYv39ebHQ5hqqobeKlr/Zw\n6dhkRg2McfnnwkOCyB4kfXAjmCrAg4MsXD4+hfwiG0d62dodVXVNbCqt9ujwwc7cMjWD2qYW3vz2\ngNfPZSStNb9etJk9lcfJzYzjH1/u4tte3AZ4Or+EphY7v7zA9avvNtIHN4apAhwgL9tKc6vm/U29\na9bgl8UVaI1Hps93Zaw1ljMz4nhp1Z6A3rj25a/38sHmcu69MIsXbpqEtV9f7n5zY6/45nGq0qP1\nvLZ2H1flWN1aX17GgxvDdAE+cmA0I5KjWbS+d7VRlhXaSIwKY9TAaJ+cb/60TA4ea+SjLeU+OZ+v\nrdt3lD99uIPzRiRx6zTHKnuPXT2eg9UN/HbJdqPL87m/f74TpRR3njvUrZ9v64PLtHrf6jLAlVKp\nSql8pdR2pdQ2pdQvnM/HKaWWKqV2Ov9vP++X65CXncLm0mPsPFzrq1MaqqXVzoriCmYMSzztpApP\nmpmVRGZiREBOr6+qa+L219eTHBvO364c/91nOjGtH7fNHMKi9aV8HKC/uDqy83Ati9eXcmNumtvL\nM3zXB98jAe5LrlyBtwC/1FqPBHKB25RSI4FfA19orYcCXzj/7hOXj08hyKJYtL53TK1fv7+amsYW\nn/S/21gsilumZrK1rCagbk612jW/eGMjVcdP8Ox1E4npG/K91+88dyhjrTE88M4WDtf0jrV3Hl1a\nTJ+QIH4+c0iP3sfRB6+RPrgPdRngWutyrfV65+NaYAeQAlwOvOI87BXgR94q8lSJUWFMH5bIOxtK\nabUH1tVhR/KLbARbFFOGJvj0vHMmpJAQGcrzATSx5/HPi1lVUskfLh/F6JQfjrQICbLw2NXjaWxu\n5b63Nwfct49TbS6t5uOth/ipc6u0nsjNjEdr+HZP4PzC93fd6oErpdKBCcBaoL/Wuu175iGgfyc/\nM18pVaCUKqio8NxCSXnZVg7XNPFVSaXH3tNf5RfamJQeR3R4SNcHe1B4SBA35KazrNBGic387ar8\nQhtPLCvhyolWrp40qNPjBidG8uCsEaworuBfqwN7o4uHPy2iX98QfurcKq0nZDy477kc4EqpSGAR\ncJfWuqb9a9pxmdLhpYrW+jmtdY7WOicx0XMTUM4dkUR0eHDA38w8WN1A4aFaj6393V03TE4jLNjC\nCyvNvW/mgSP13PXmRkYkR/OHH43u8vjrc9OYkZXInz/aERC/vDry9a5KVu6s5LaZQ4jywMWB9MF9\nz6UAV0qF4Ajv17TWi51PH1ZKJTtfTwZ8Oj0yPCSIS8cN5NNthwJ62FfbrFNf9r/bi4sI5YqJVhav\nL6OitsmQGnqqqaWV215fj92uefa67B/sLNMRpRQP5Y2lb2gQd725MeBmaWqtefjTIgZE/3CrtJ6Y\nnJkgfXAfcmUUigJeBHZorR9t99IS4Cbn45uA9zxf3unlZVtpbLbz8ZZDvj61z+QXVmDt14fBiZGG\n1TDv7Aya7XZeXb3XsBp64vfvb2dz6TEeuWoc6d0Y45wUHc7/zh3L1rIaHv8isGZpfrHDxob9HW+V\n1hO5mXHSB/chV67ApwA3AOcopTY6/8wC/gKcr5TaCZzn/LtPZQ+KJSMhgrcDtI3S2NzKVyWVnDM8\nyaVV4bwlMzGS80b059U1+2g4Ya59M9/ZUMpra/fzn9MzuXDUgG7//EWjB3DlRCvPLt8VMJNU7M6t\n0jISIrhi4g+3SuuJcamxhEkf3GdcGYWySmuttNZjtdbjnX8+0lpXaa3P1VoP1Vqfp7X2+b9upRRz\nJ6TwzZ4jHDhS7+vTe903e47Q0Nzqk9mXXZk/LZOj9c2m+mVZdKiWBxZv4cyMOO5zY3p4m/+ZPcox\nS3NhYMzSXLLpIIWHarn7/GGEeHhZYkcfvJ/0wX3EdDMxTzUnOwWAxQE4JnxZoY2wYAuTB8cbXQo5\naf0YnxrLiyt3m2LoZm1jMz/79zqiwkN48scTerR+umOW5jjKjjbw+/fNPUuzudXOo0uLGZEczaVj\nOt8qrSe+Gw9eb/5fdv7O9AFu7deXyZnxLN4QeNutLS+ycdbgeI/2KN2llGNiz96qepZuP2x0Oael\nteZXizaz70g9T107gaSo8B6/58S0OH4+YwhvrSvlk63mnaX55rcH2H+knvsuHOa1Wb1tffBvAqTl\n5M9MH+AAc7NT2FdVz7p9R40uxWN2V9Sxt6resNEnHblwVH9S4/r4/Y49L67aw0dbDnH/hVmcmem5\nby+/OG8oY1JieGDxFmwmnKXZcKKVJ77YSU5aP6+25aQP7jsBEeAXj0mmT0hQQI0Jzy9yTHqa4Qf9\n7zbBQRb+Y0oGBfuOsn6/f/6yLNh7hL98XMgFI/szf1qmR9+7bZZmQ3Mr9y8y3yzNf63ei622ifsv\nGu7Vm+Lf9cElwL0uIAI8MiyYi0cP4INN5TQ2m2uURGfyC20MTYokNa6v0aV8z1U5qUSH++e+mZV1\nTdz2+nqs/frwyFXjvBJSQ5Ii+a9ZI1heVMG/15hnlmZNYzPPfrmL6cMSOSPDta3SeiI3M57t5dIH\n97aACHBw7Fpf29TCZ37en3XF8aYW1u6pYqYftU/aRIQFc11uGp9sPcT+Kv8Z+dNq19y5YAPV9c08\nc91Ery47cENuGtOHJfKnj3awq6LOa+fxpBdW7Ka6vpn7urFVWk9IH9w3AibAczPjSY4JZ3EAtFG+\nKqmkuVX7xfDBjtx8VjpBFsVLX/nP9PpHlxbx9a4q/vij0Yz08prpSikevmIsfUKCuPvNjX6/6UVl\nXRMvrNrDJWOTO1zAyxvGD5I+uC8ETIAHWRRzJqSworjClDeY2ssvshEZFkxOus+WWO+W/tHhXD4+\nhTe/PUB1vfFb232x4zBP5+/imkmpXJmT6pNzOmZpjmFz6TGe+GKnT87prrat0u45f5jPzhkWHMTE\nNOmDe1vABDjA3Gwrdg3vbjTvmHCtNfmFFUwdmuDxSRae9NOpGTQ0t/La2v2G1rG/qp6739zIqIHR\n/Hb2KJ+e+6LRyVwx0crT+SWs2+efrYKy6gZeW7OfK7KtPl+OQfrg3ue/CeGGIUmRjEuNZdG6MtON\nEGizo7yWQzWNftn/bm/4gGimDUvk5a/30tRizI3jxuZWfv76OgCevW6iIePl/+eykQyM7cPdb26i\nrqnF5+fvyuOfO9Zw+cV57m2V1hNt64NLH9x7AirAAa7ITqHocC3bDtZ0fbAfalt9cEaWMcvHdsct\nUzOoqG3ivY3GbDD9u/e3sbWshseuHs+geGNG60SFh/DY1eMpPVrP79/fZkgNnSmx1fH2ulKuz01j\nYKx7W6X1xLjUGOmDe1nABfhl4wYSGmQx7Zjw/EIbY1JiPDJ70NvOHpLA8AFRvGDAvplvrytlwTcH\n+PmMwZw7osO9RHxmUnoct04fzMKCUj7d5j8rYz7m3CrttpmDDTl/Wx9cNjr2noAL8Ni+oZw7Iokl\nGw/6/eiAU1XXn2D9/qPMNMHVN5ycXl98uI4viz2321JXdpTX8OA7W5icGe/TG3Onc9d5wxidEu2Y\npVlr/E30rWXH+HBLOfOmZhIfGWZYHbmZ8ew4VOMXN7sDUcAFODhuZlYdP8GXRb4LFU/4srgCu8bv\n+9/tXTZuIP2jw3y2Y0+Nc5GqmD4hPHFtzxap8qTQYAt/v3o8x5tauN8P9tJ86NMiYj20VVpPfNcH\nl/XBvcI//vV72IysROIjQk3XRlleVEFcRChjrbFGl+Ky0GALN5+VwaqSSrYdPObVc2mtuXfhJg4c\nbeDp67JJjDLuyrIjQ5KieODi4Y5ZmgaOzlmzu4oVxRX8fMZgn++jeqqTfXAJcG8IyAAPCbIwe/xA\nvthhM81Xt1a7ZnmRjRnDEgny0ipx3vLjMwcRERrEi16+Cn9+5W4+236YBy4ezqR0708Hd8eNk9OZ\nOjSBP3243ZBZmm1bpfWPDuPGyek+P/+pZDy4dwVkgINju7UTrXbe32yOpT83lVZztL6ZGSZqn7SJ\n6RPC1ZMGsWTTQcqPNXjlHGt3V/HXT4qYNWYA8842ti1wOhaL4pErxxFu0CzN/CIb6/Yd5c5zPbtV\nWk9IH9x7AjbARw2MJqt/FIvWmaONkl9ow6Jg+lBz3MA81U+mpGPXmpe/3uvx97bVNnL7gg2kxfXl\nr3ljDd1ezhX9o8P58xzHLM0nfThL027XPPxpMWnxfbnKRzNSXSF9cO8J2ABXSpE3MYWNB6pNseBQ\nfpGNiWn9iOlrbM/SXalxfZk1JpnX1+z36LZjLa127nh9A7WNzTxzfTZRBvd0XTVrTDJ52Vaeyi/x\n2Tr1728+yI7yGu7xwlZpPTEuNYbwEOmDe4P//Ff2gh+NT8Gi8PsFrmw1jWwtqzHV6JOO3DI1k9qm\nFt789oDH3vORz4pZu+cIf54zhuEDvLtIlaf9drZjluY9Czdy3MuzNJtb7Ty2tJjhA6K4bOxAr56r\nu6QP7j0BHeBJ0eFMHZrIO+vLsPvxPo7LncMd/XX1QVeNS43ljIw4/vnVXlo80Pv9bNsh/vHlLq47\ncxBzsz27e7ovRIWH8OhV49l/pJ4/fODdvTTfKihlb1U9912Y5bWt0noiN0P64N4Q0AEOjnXCDx5r\nZLUf//ZfVmgjOSac4QOijC6lx26ZmklZdQMfbe3ZjMR9Vcf55VubGGuN4TeXjfRQdb53RoZjluYb\n3x7w2l6ijc2tPP5FMdmDYv1qC772cgdLH9wbAj7ALxjZn6iwYL8dE36ixc6qkkpmZCX5/c05V5w7\nPInMhAieX+H+9PrG5lZu/fd6LErx9I+zCQv2j9EU7rr7vGGMGhjNrxdtpqK2yePv/+rqfRyu8f5W\naT0x1urog/vzhZQZBXyAh4cEccnYZD7ZesjrfUh3FOw9Ql1Ti99eOXWXxaL46dRMtpQdY62bV1u/\neW8rO8pr+PvV4/1uSzl3tM3SrGtq4Vce3kuztrGZZ5aXMG1YIrke3MDZ0072weUK3JMCPsDB0Uap\nP9HKxz38Wu8N+UU2QoMsnDXYf//H111zs1OIjwh1a9/Mhd8eYGFBKXecM8T0N3XbG9o/il9fPJxl\nhTZe/8ZzszRfWLmHo/XN3HeBb7ZK64ncjHgKpQ/uUb0iwHPS+pEW39cvx4QvK7RxZmYcEWHBRpfi\nMeEhQdwwOY3Pd9gosbk+hHNr2TH++72tnD0kgbvO849FqjzpJucszT9+sIPdHhjaWlXXxAsrdzNr\nzADGWH2zVVpPtPXB3f1mJn6oVwS4Uoq5E6ys3l1F6VH/2Yh3f1U9uyqOm370SUduyE0jLNjCi6tc\nm15/rKGZn7+2nriIUB6/ZrzplhNwhcWiePiKcYQGW7h74aYez9J8ZvkuGppbued8/7/6hpN9cBlO\n6Dm9IsDB8bUe4N0N/rPdWtvmDYHS/24vPjKMvIlWFq0vpbLu9Dfu7HbNLxdu4mB1A0/9ONvQ5U+9\nbUCMY5bmpgPVPLWsxO33OVjdwKtr9pGXbWVIkm+3SnOX9ME9r9cEeGpcX87IiGPRev/Zbi2/yEZG\nQgTpCRFGl+IV887O4ESLnX+t3nfa4/5vxW4+33GYBy8ZwcQ0/9zI2ZMuGZvM3AkpPJVfwvr97s3S\nfOKLnaCN2SqtJ6QP7lldBrhS6iWllE0ptbXdc+OVUmuUUhuVUgVKqTO8W6ZnXJFtZU/lcTYcqDa6\nFBpOtLJ6V1VAtk/aDE6M5LwR/fn3mn00nOh438zVu6p4+NNCLhmbzM1npfu2QAP99vJRDIgO5+43\nuz9Lc3dFHW+tK+XHZw7C2s9co3QmSx/co1y5An8ZuOiU5x4Cfqe1Hg/8xvl3v3fxmAGEh1j84mbm\n6t2VNLXYmTncnItXuWr+tEyOHD/R4Th8W00jdyzYQEZChCkWqfKk6PAQHr1qHPuP1PPHD7s3S/PR\npcWEBVu4/ZwhXqrOe8ZaY6UP7kFdBrjWegVw6q9LDbQtTBEDGLOrbTdFhYdw4agBvL/pII3Nxuyk\n3mZZoY2+oUGckeGf61p7yqT0foyzxvDiqj3fW86gudXOba+v53hTC89eP5HIABqF46ozM+OZPy2T\nBd+4Pktza9kxPthczryzM0gw4b2C0GALOWlx0gf3EHd74HcBDyulDgCPAA90dqBSar6zzVJQUWH8\nFmd52VZqGltYVmgzrAatNfmFFUwZkmD6WYZdUUpxy7RM9lQe5/MdJ0PqoU8K+XbvUf6SN4Zh/c2/\nhIC77jl/GCOSXZ+l+chnRcT0CeGnUzN9UJ135GbGsaO8hqPHpQ/eU+4G+M+Au7XWqcDdwIudHai1\nfk5rnaO1zklMNL5dMGVIAv2jwwxto+y01VFW3RDQ/e/2Lho1gJTYPt/tm/nJ1nKeX7mHGyencfn4\nFIOrM1ZYcBCPXzOe2qYWft3FLM1v9hxheVEFP5sxmJg+5lhWtyNtM0alD95z7gb4TcBi5+O3AFPc\nxAQIsih+NCGF5cUVXlmXwhX5zqv/QO9/twkOsjDv7Ay+2XuEdzeUcd9bmxmXGsuDl4wwujS/MKx/\nFL+6aDhfFNpY8E3HS/E6tkorJCkqjJv8YKu0npA+uOe4G+AHgenOx+cAvtt2xAOuyLbSate8t9GY\nMeHLCm0MHxBFckwfQ85vhKsmpRIVHsxdb24kOEjxzHXmX6TKk35yVjpnD0ngDx9sZ0/l8R+8vry4\ngm/3HuWOc4fSJ9Tcn9vJPrgEeE+5MoxwAbAayFJKlSql5gG3AH9TSm0C/gzM926ZnjW0fxRjrTEs\nXu/7AK9pbKZg39GAnLxzOpFhwdw4OQ2l4O/XTCAltvf88nJF216aocEW7n5z4/fWU7fbNQ9/UsSg\nuL5c7UdbpfVEbmYchYdqpQ/eQ66MQrlWa52stQ7RWlu11i9qrVdprSdqrcdprc/UWq/zRbGeNHdC\nCtvLa9hRXuPT867aWUmrXQfUQk2uuuf8LFbcN5Ppw3pH66i7BsSE86c5o9l4oJqn8k/O0vxwSznb\nnVulhQYHxtw76YN7RmD8a3DD7PEphAQpn9/MXFZoI6ZPCBNSY316Xn8QZFEBsTysN106diBzJqTw\n5LISNuw/SkurnUeXFpPVP4rLxvnXVmk9IX1wz+i1AR4XEcrMrCTe3XjQI9t/ucJu1ywvqmDasESC\n/WjTWeFffuecpXnPwk38a/U+9lQe594LswJqgS/pg3tGr06RvIlWKuuaWLmz0ifn23rwGJV1TZzT\nS0afCPdEh4fwt6vGsbfqOL//YDsTBsVy3ojAa7lNHhwvffAe6tUBPjMriX59Q3jbR9ut5RdWoBRM\nGyoBLk4v1zlLE+C+C7MCcpmB3EzHLGTpg7uvVwd4aLCF2eMGsnT7YY41NHv9fMuKbIxPjQ3o5VKF\n5/z6ouGsuG8mZw1OMLoUrxiTEkufkCBpo/RArw5wcLRRTrTY+XBzuVfPU1nXxObS6l4z+1L0nFKK\nQfGBe9M3NNhCTno/CfAe6PUBPiYlhiFJkV7ftf7Logq0DszNG4RwV26mow9+RPrgbun1Aa6UIi/b\nyrp9RzucAecp+UU2EqPCGJkc3fXBQvQSbX3wb/bIVbg7en2AA8yZkIJFwTteugpvabWzoriCmVmJ\nWAJoKJgQPXWyDy43Mt0hAY5jBtyUIQksWl/2vTWrPWX9/mpqGluk/y3EKaQP3jMS4E552VbKqhu8\nMqRpWaGNYIvi7KGBOZpAiJ6QPrj7JMCdLhw1gMiwYBZ7oY2yvMjGpPQ4osLNu4azEN4ifXD3SYA7\n9QkNYtaYAXy0pZz6E93bZPZ0yqobKDxUK6NPhOiE9MHdJwHeztxsK8dPtPLptkMee8/lRb1r8wYh\nukv64O6TAG/njPQ4rP36eHSd8PxCG6lxfRicGOmx9xQi0Egf3D0S4O1YLIq52VZWlVRSfqyhx+/X\n2NzKVyVVzMxKCsi1LITwlLb1waUP3j0S4KfIy05Ba3hnQ8+vwtfuOUJDc2uv3LxBiO4Ya42hT0gQ\nq3dJgHeHBPgp0uIjyEnrx6J1pafdIdwV+YU2wkMsTHZeXQghOhYS1NYHlxuZ3SEB3oG8iVZ2VRxn\nc+kxt99Da01+kY2zBicQHmLuTWiF8IXczHiKDtdSVddkdCmmIQHegUvGJhMabOnRAld7Ko+zr6qe\nmVky+kQIV5zsg8tVuKskwKPeIlYAAA9+SURBVDsQHR7CBSP7s2TTQZpaWt16j2WFjuGDM2T6vBAu\naeuDy3BC10mAdyJvopXq+mbyCyvc+vnlRRUMTYqUTXyFcJH0wbtPArwTU4ckkBgV5lYbpa6phbV7\nqmT0iRDdJH3w7pEA70RwkIU5E1LIL7R1+x/TVyWVNLdqWX1QiG6SPnj3SICfxtzsFFrsmiWbDnbr\n5/ILbUSFBZOT3s9LlQkRmMZaY+gbKn1wV0mAn8bwAdGMGhjdran1bcMHpw5LICRIPl4husPRB4+T\nPriLJGG6kJdtZUvZMYoP17p0/PbyGg7XNMnoEyHclJsZJ31wF0mAd2H2+IEEWxSL1rl2M3N5kWPU\nygwZ/y2EW9r64N7YXCXQSIB3ISEyjBlZibyzoYxWF7ZbW1ZoY0xKDElR4T6oTojAMyZF+uCu6jLA\nlVIvKaVsSqmtpzx/h1KqUCm1TSn1kPdKNF5ethVbbROrSipPe9zR4yfYsP+oDB8UogdO9sElwLvi\nyhX4y8BF7Z9QSs0ELgfGaa1HAY94vjT/cc6IJGL6hHTZRlmxswK7RqbPC9FDuZlxFB+uo1L64KfV\nZYBrrVcApzajfgb8RWvd5DzG5oXa/EZYcBCXjUvm022HqG1s7vS4/EIb8RGhjLPG+rA6IQKPjAd3\njbs98GHAVKXUWqXUl0qpSZ0dqJSar5QqUEoVVFS4Ny3dH+RlW2lqsfPRlvIOX2+1a74srmD6sEQs\nFtm8QYiekD64a9wN8GAgDsgF7gMWqk62nNFaP6e1ztFa5yQmmre1MD41lszECBat63hM+MYD1Ryt\nb5b+txAeIH1w17gb4KXAYu3wDWAHEjxXlv9RSpGXbeWbvUfYX1X/g9eXF9kIsiimDTXvLykh/In0\nwbvmboC/C8wEUEoNA0KB0w/RCABzJqSgFCze8MObmcsKbUwc1I+YviEGVCZE4JksffAuuTKMcAGw\nGshSSpUqpeYBLwGZzqGFbwA36Z7uP2YCA2P7cNbgeBavL/vedmuHaxrZdrCGGcPl6lsITxmdEkOE\n9MFPK7irA7TW13by0vUersUU8rKt3LNwE9/uPcoZGXGAo30CcI70v4XwmLY+uGx03DmZidlNF44a\nQN/QIBa3Wyc8v7CC5JhwsvpHGViZEIEnNzOenTbpg3dGArybIsKCuXh0Mh9uLqexuZUTLXZWlVQy\nc3gSnQzEEUK4KTfT8S13raxO2CEJcDfkTUyhtqmFT7cdomDvEeqaWmTzBiG8QPrgp9dlD1z8UG5G\nPCmxfVi8voyhSZGEBlmYMiTe6LKECDgyHvz05ArcDRaLYs6EFFburOCDzeWcmRlH31D5XSiEN0gf\nvHMS4G6am52CXcOhmkYZfSKEF0kfvHMS4G7KTIwke5Bj0SrpfwvhPdIH75x87++Be87PYlmhjfSE\nCKNLESJgSR+8c3IF3gNnD03gN5eNNLoMIQLe5MHSB++IBLgQwu99t0+m9MG/RwJcCOH3Rg+MJiI0\niNW7A37NvG6RABdC+L3gIAuTMuJYI1fg3yMBLoQwhdzMeEpsdVTUSh+8jQS4EMIUvuuD75HRKG0k\nwIUQptDWB5fhhCdJgAshTEH64D8kAS6EMA3pg3+fBLgQwjSkD/59EuBCCNMYPTCayLBg6YM7SYAL\nIUwjOMjCpPR+0gd3kgAXQphKWx/cVttodCmGkwAXQpiKrItykgS4EMJURkkf/DsS4EIIUznZB5cA\nlwAXQphObmY8uyqO9/o+uAS4EMJ0pA/uIAEuhDAd6YM7SIALIUxH+uAOEuBCCFOSPrgLAa6Uekkp\nZVNKbe3gtV8qpbRSKsE75QkhRMcmD5Y+uCtX4C8DF536pFIqFbgA2O/hmoQQoksjk6OJ6uV98C4D\nXGu9AujoV9xjwP2A9nRRQgjRlbb1wVdLgHePUupyoExrvcmFY+crpQqUUgUVFRXunE4IITqUmxnH\n7orj2Gp6Zx+82wGulOoL/BfwG1eO11o/p7XO0VrnJCYmdvd0QgjRqbbx4Gv29M4+uDtX4IOBDGCT\nUmovYAXWK6UGeLIwIYToSm/vgwd39we01luApLa/O0M8R2td6cG6hBCiSyf3yeydAe7KMMIFwGog\nSylVqpSa5/2yhBDCNb25D97lFbjW+touXk/3WDVCCNFN7fvgs8cNNLga35KZmEIIU+vNfXAJcCGE\nqfXmPrgEuBDC9CZnxvfKPrgEuBDC9HrreHAJcCGE6Y0c6OiDr97Vu9ooEuBCCNMLsijOyIhjbS/r\ng0uACyECQm5mPLsrj7O7os7oUnxGAlwIERAuGNWfyLBg8p79mvwim9Hl+IQEuBAiIKTFR7Dk9in0\njw7nJ//8loc+KaSl1W50WV4lAS6ECBiZiZG8e9sUrpmUyjPLd/HjF9ZyOICHFkqACyECSnhIEH/J\nG8ujV41jS+kxZj2+klU7A3OtPQlwIURAmpttZcntU4iLCOWGl9by2NJiWu2BtYGYBLgQImAN7R/F\ne7dPYc74FB7/Yic3vrSWitomo8vyGAlwIURA6xsazN+uGsdf88ZQsPcolzyxMmDGi0uACyECnlKK\nqycN4t3bphARFsy1z6/h6fwS7CZvqUiACyF6jRHJ0Sy5fQqzxiTz8KdF/Mcr33L0+Amjy3KbBLgQ\noleJCg/hyWsn8IfLR/F1SRWznljJun3mXARLAlwI0esopbhhcjqLfnYWwUGKq/9vDS+s3I3W5mqp\nSIALIXqtMdYYPrhjKueOSOKPH+5g/qvrOFbfbHRZLpMAF0L0ajF9QvjH9RP570tHkl9o45InV7K5\ntNroslwiAS6E6PWUUsw7O4OFt07Gbtdc8exqXvl6r9+3VCTAhRDCKXtQPz68cypnD03gf5Zs4/bX\nN1Db6L8tFQlwIYRop19EKC/cmMOvLx7OJ9sOcdmTq9h28JjRZXVIAlwIIU5hsShunT6YBbfk0tDc\nypxnvmbBN/v9rqUiAS6EEJ04IyOOD++cypkZcTyweAv3LNzE8aYWo8v6jgS4EEKcRkJkGC//5Azu\nOX8Y724sY/ZTqyg+XGt0WYAEuBBCdCnIorjz3KG8Nu9MjjW0MPupVby9rtTosiTAhRDCVWcNSeCj\nO89mnDWWe9/axP1vb6LhRKth9UiACyFENyRFh/PaT8/k9plDWFhQypxnvmJXRZ0htXQZ4Eqpl5RS\nNqXU1nbPPayUKlRKbVZKvaOUivVumUII4T+Cgyzce2EWL/9kEodrGpn95Cre21jm8zpcuQJ/Gbjo\nlOeWAqO11mOBYuABD9clhBB+b0ZWEh/eOZXhydH84o2NPPjOFhqbfddS6TLAtdYrgCOnPPeZ1rpt\nLM0awOqF2oQQwu8NjO3DG/Nz+c9pmby2dj95z37NvqrjPjm3J3rg/wF83NmLSqn5SqkCpVRBRUWF\nB04nhBD+JSTIwgOzRvD8jTkcOFLPpU+s4pOt5V4/b48CXCn1INACvNbZMVrr57TWOVrrnMTExJ6c\nTggh/Nr5I/vz4Z1TyUyM4NZ/r+d372/jRIvda+dzO8CVUjcDlwLXaX+bXyqEEAZJjevLW7eexc1n\npfPPr/Zy5f+tpvRovVfO5VaAK6UuAu4HZmutvVOZEEKYVGiwhd/OHsUz12Wz21bHJU+sYs3uKo+f\nx5VhhAuA1UCWUqpUKTUPeAqIApYqpTYqpf7h8cqEEMLkZo1J5v07zmasNYa0+L4ef3/ly+5HTk6O\nLigo8Nn5hBAiECil1mmtc059XmZiCiGESUmACyGESUmACyGESUmACyGESUmACyGESUmACyGESUmA\nCyGESUmACyGESfl0Io9SqgLY5+aPJwCVHizHU6Su7pG6ukfq6h5/rQt6Vlua1voHqwH6NMB7QilV\n0NFMJKNJXd0jdXWP1NU9/loXeKc2aaEIIYRJSYALIYRJmSnAnzO6gE5IXd0jdXWP1NU9/loXeKE2\n0/TAhRBCfJ+ZrsCFEEK0IwEuhBAm5XcBrpS6SClVpJQqUUr9uoPXpyml1iulWpRSV/hRXfcopbYr\npTYrpb5QSqX5SV23KqW2OHdOWqWUGukPdbU7Lk8ppZVSPhn65cLndbNSqsL5eW1USv3UH+pyHnOV\n89/YNqXU6/5Ql1LqsXafVbFSqtpP6hqklMpXSm1w/m9ylp/UlebMh81KqeVKKWuPTqi19ps/QBCw\nC8gEQoFNwMhTjkkHxgL/Aq7wo7pmAn2dj38GvOkndUW3ezwb+MQf6nIeFwWsANYAOf5QF3Az8JQv\n/l11s66hwAagn/PvSf5Q1ynH3wG85A914bhh+DPn45HAXj+p6y3gJufjc4BXe3JOf7sCPwMo0Vrv\n1lqfAN4ALm9/gNZ6r9Z6M2D3s7ry9ckNntcAPfvN6rm6atr9NQLwxV3rLuty+gPwV6DRBzV1py5f\nc6WuW4CntdZHAbTWNj+pq71rgQV+UpcGop2PY4CDflLXSGCZ83F+B693i78FeApwoN3fS53PGa27\ndc0DPvZqRQ4u1aWUuk0ptQt4CLjTH+pSSmUDqVrrD31Qj8t1OeU5v+K+rZRK9ZO6hgHDlFJfKaXW\nKKUu8pO6AEdrAMjgZDgZXddvgeuVUqXARzi+HfhDXZuAuc7Hc4AopVS8uyf0twA3PaXU9UAO8LDR\ntbTRWj+ttR4M/Ar4f0bXo5SyAI8CvzS6lg68D6RrrccCS4FXDK6nTTCONsoMHFe6zyulYg2t6Puu\nAd7WWrcaXYjTtcDLWmsrMAt41fnvzmj3AtOVUhuA6UAZ4PZn5g//D7VXBrS/4rE6nzOaS3Uppc4D\nHgRma62b/KWudt4AfuTVihy6qisKGA0sV0rtBXKBJT64kdnl56W1rmr33+4FYKKXa3KpLhxXc0u0\n1s1a6z1AMY5AN7quNtfgm/YJuFbXPGAhgNZ6NRCOYzEpQ+vSWh/UWs/VWk/AkRVord2/8evtxn43\nbwIEA7txfBVruwkwqpNjX8Z3NzG7rAuYgOMGxlB/+rza1wNcBhT4Q12nHL8c39zEdOXzSm73eA6w\nxk/qugh4xfk4AcdX9Xij63IeNxzYi3NioJ98Xh8DNzsfj8DRA/dqfS7WlQBYnI//BPy+R+f0xQfe\nzQ9hFo6ri13Ag87nfo/jqhZgEo6rkeNAFbDNT+r6HDgMbHT+WeIndT0ObHPWlH+6IPVlXacc65MA\nd/Hz+l/n57XJ+XkN95O6FI6203ZgC3CNP9Tl/Ptvgb/4op5ufF4jga+c/x03Ahf4SV1XADudx7wA\nhPXkfDKVXgghTMrfeuBCCCFcJAEuhBAmJQEuhBAmJQEuhBAmJQEuhBAmJQEuhBAmJQEuhBAm9f8B\nptg3qAh6/KgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRwLeavJl1dA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e15f63ab-1286-4eb5-e2d9-313fd092c729"
      },
      "source": [
        "print(min(loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11.81053907731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peh32BbyVa4A",
        "colab_type": "text"
      },
      "source": [
        "# `Drop out Hidden Layer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJg_55b7Vmau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers\n",
        "\n",
        "loss = []\n",
        "import numpy\n",
        "\n",
        "myPct = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "\n",
        "for i in range(0, len(myPct)):\n",
        "  \n",
        "\n",
        "\n",
        "    model33 = Sequential()\n",
        "\n",
        "    model33.add(Dense(15, activation='relu',input_shape=(13,)))\n",
        "\n",
        "    model33.add(Dense(15, activation=\"relu\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model33.add(Dense(1))\n",
        "\n",
        "    model33.summary()\n",
        "\n",
        "    model33.compile(loss='mae', optimizer=Adam(lr=0.01))\n",
        "    callbacks = []\n",
        "    history = model3.fit(x_train2, y_train, batch_size=5, epochs=50, callbacks=callbacks, verbose=0)\n",
        "\n",
        "    model33.evaluate(x_test2, y_test)\n",
        "\n",
        "    myLoss = model33.evaluate(x_test2, y_test)\n",
        "    loss.append(myLoss)\n",
        "    print(loss)\n",
        "\n",
        "\n",
        "\n",
        "print(myPct)\n",
        "print(loss)\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "  \n",
        "plt.plot(myPct, loss)\n",
        "\n",
        "print(min(loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrWdnGFRV0Dj",
        "colab_type": "text"
      },
      "source": [
        "## L2 Regularizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-12-26T20:17:15.317837Z",
          "start_time": "2018-12-26T20:17:06.136739Z"
        },
        "scrolled": true,
        "id": "xH5H3DDlXsia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c058c96-7faa-42cf-cfdd-333dc1328f88"
      },
      "source": [
        "# Improve Variance by using L2.\n",
        "\n",
        "from keras import regularizers\n",
        "\n",
        "loss = []\n",
        "import numpy\n",
        "\n",
        "myAlpha = list(numpy.arange(0, 0.1, 0.0005))\n",
        "myAlpha1 = [ 0.00001,0.0001, 0.001,0.01]\n",
        "\n",
        "for i in range(0, len(myAlpha1)):\n",
        "\n",
        "    model22 = Sequential()\n",
        "    model22.add(Dense(15, activation=\"relu\", input_shape=(13,)))\n",
        "    myRegularizer1 = regularizers.l2(myAlpha1[i])\n",
        "    \n",
        "    model22.add(Dense(15, activation=\"relu\", kernel_regularizer=myRegularizer1 ,bias_regularizer=myRegularizer1 ))\n",
        "    model22.add(Dense(1))\n",
        "\n",
        "    model22.summary()\n",
        "\n",
        "    model22.compile(loss='mae', optimizer=Adam(lr=0.01))\n",
        "    callbacks = []\n",
        "    history = model22.fit(x_train2, y_train, batch_size=5, epochs=50, callbacks=callbacks, verbose=0)\n",
        "\n",
        "    myLoss = model22.evaluate(x_test2, y_test)\n",
        "    loss.append(myLoss)\n",
        "    print(loss)\n",
        "\n",
        "\n",
        "\n",
        "print(myAlpha)\n",
        "print(loss)\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "  \n",
        "plt.plot(myAlpha, loss)\n",
        "\n",
        "print(min(loss))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_262\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_771 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_772 (Dense)            (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_773 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 466\n",
            "Trainable params: 466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 58ms/step\n",
            "[24.415749643363206]\n",
            "Model: \"sequential_263\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_774 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_775 (Dense)            (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_776 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 466\n",
            "Trainable params: 466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 57ms/step\n",
            "[24.415749643363206, 26.910958832385493]\n",
            "Model: \"sequential_264\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_777 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_778 (Dense)            (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_779 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 466\n",
            "Trainable params: 466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 58ms/step\n",
            "[24.415749643363206, 26.910958832385493, 23.67594550637638]\n",
            "Model: \"sequential_265\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_780 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_781 (Dense)            (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_782 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 466\n",
            "Trainable params: 466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 59ms/step\n",
            "[24.415749643363206, 26.910958832385493, 23.67594550637638, 21.313645942538393]\n",
            "Model: \"sequential_266\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_783 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_784 (Dense)            (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_785 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 466\n",
            "Trainable params: 466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 58ms/step\n",
            "[24.415749643363206, 26.910958832385493, 23.67594550637638, 21.313645942538393, 24.71209772895364]\n",
            "Model: \"sequential_267\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_786 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_787 (Dense)            (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_788 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 466\n",
            "Trainable params: 466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 58ms/step\n",
            "[24.415749643363206, 26.910958832385493, 23.67594550637638, 21.313645942538393, 24.71209772895364, 30.93883929533117]\n",
            "Model: \"sequential_268\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_789 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_790 (Dense)            (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_791 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 466\n",
            "Trainable params: 466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 59ms/step\n",
            "[24.415749643363206, 26.910958832385493, 23.67594550637638, 21.313645942538393, 24.71209772895364, 30.93883929533117, 24.877664902630976]\n",
            "Model: \"sequential_269\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_792 (Dense)            (None, 15)                210       \n",
            "_________________________________________________________________\n",
            "dense_793 (Dense)            (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_794 (Dense)            (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 466\n",
            "Trainable params: 466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "102/102 [==============================] - 6s 59ms/step\n",
            "[24.415749643363206, 26.910958832385493, 23.67594550637638, 21.313645942538393, 24.71209772895364, 30.93883929533117, 24.877664902630976, 23.644640417659986]\n",
            "[0.0, 0.0005, 0.001, 0.0015, 0.002, 0.0025, 0.003, 0.0035, 0.004, 0.0045000000000000005, 0.005, 0.0055, 0.006, 0.006500000000000001, 0.007, 0.0075, 0.008, 0.0085, 0.009000000000000001, 0.0095, 0.01, 0.0105, 0.011, 0.0115, 0.012, 0.0125, 0.013000000000000001, 0.0135, 0.014, 0.0145, 0.015, 0.0155, 0.016, 0.0165, 0.017, 0.0175, 0.018000000000000002, 0.0185, 0.019, 0.0195, 0.02, 0.0205, 0.021, 0.021500000000000002, 0.022, 0.0225, 0.023, 0.0235, 0.024, 0.0245, 0.025, 0.025500000000000002, 0.026000000000000002, 0.0265, 0.027, 0.0275, 0.028, 0.0285, 0.029, 0.029500000000000002, 0.03, 0.0305, 0.031, 0.0315, 0.032, 0.0325, 0.033, 0.0335, 0.034, 0.0345, 0.035, 0.035500000000000004, 0.036000000000000004, 0.0365, 0.037, 0.0375, 0.038, 0.0385, 0.039, 0.0395, 0.04, 0.0405, 0.041, 0.0415, 0.042, 0.0425, 0.043000000000000003, 0.043500000000000004, 0.044, 0.0445, 0.045, 0.0455, 0.046, 0.0465, 0.047, 0.0475, 0.048, 0.0485, 0.049, 0.0495, 0.05, 0.0505, 0.051000000000000004, 0.051500000000000004, 0.052000000000000005, 0.0525, 0.053, 0.0535, 0.054, 0.0545, 0.055, 0.0555, 0.056, 0.0565, 0.057, 0.0575, 0.058, 0.0585, 0.059000000000000004, 0.059500000000000004, 0.06, 0.0605, 0.061, 0.0615, 0.062, 0.0625, 0.063, 0.0635, 0.064, 0.0645, 0.065, 0.0655, 0.066, 0.0665, 0.067, 0.0675, 0.068, 0.0685, 0.069, 0.0695, 0.07, 0.07050000000000001, 0.07100000000000001, 0.07150000000000001, 0.07200000000000001, 0.0725, 0.073, 0.0735, 0.074, 0.0745, 0.075, 0.0755, 0.076, 0.0765, 0.077, 0.0775, 0.078, 0.0785, 0.079, 0.0795, 0.08, 0.0805, 0.081, 0.0815, 0.082, 0.0825, 0.083, 0.0835, 0.084, 0.0845, 0.085, 0.0855, 0.08600000000000001, 0.08650000000000001, 0.08700000000000001, 0.08750000000000001, 0.088, 0.0885, 0.089, 0.0895, 0.09, 0.0905, 0.091, 0.0915, 0.092, 0.0925, 0.093, 0.0935, 0.094, 0.0945, 0.095, 0.0955, 0.096, 0.0965, 0.097, 0.0975, 0.098, 0.0985, 0.099, 0.0995]\n",
            "[24.415749643363206, 26.910958832385493, 23.67594550637638, 21.313645942538393, 24.71209772895364, 30.93883929533117, 24.877664902630976, 23.644640417659986]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-cc17f1f00f26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyAlpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2787\u001b[0m     return gca().plot(\n\u001b[1;32m   2788\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2789\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \"\"\"\n\u001b[1;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (200,) and (8,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8CWpCYMvSU0YfElq4n8BzPZcum6w2goAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-12-26T20:17:16.425618Z",
          "start_time": "2018-12-26T20:17:15.322544Z"
        },
        "id": "hn5rhbujXsih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef161b6c-09d2-45c6-eef3-9e2e5352578d"
      },
      "source": [
        "print(min(loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21.313645942538393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE7vU5WbXsim",
        "colab_type": "text"
      },
      "source": [
        "## Problem 6\n",
        "\n",
        "Using the same number of epochs and training set, try to add regularization (any method except changing the training set) to improve the test set accuracy.  Describe the regularization methods you used (including the parameters you tried).  Which methods worked the best?  Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LijYVLciXsin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source of Code: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
        "\n",
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [10, 50, 100]\n",
        "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs, neurons = neurons)\n",
        "grid = GridSearchCV(estimator=model1, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(x_test2, y_test)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YEhvx35Xsis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}